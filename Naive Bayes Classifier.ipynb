{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f11ca1d",
   "metadata": {},
   "source": [
    "# Building a Naive Bayes Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab9c5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e4e442",
   "metadata": {},
   "source": [
    "### The path of my input file. \n",
    "(to run on your system use the input path on your computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b38baa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/renitasequeira/Desktop/detect-ai-generated-essay\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "de7d31bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In here\n",
      "/Users/renitasequeira/Desktop/Python/LLM - Detect AI generated text/input/.DS_Store\n",
      "/Users/renitasequeira/Desktop/Python/LLM - Detect AI generated text/input/train_prompts.csv\n",
      "/Users/renitasequeira/Desktop/Python/LLM - Detect AI generated text/input/test_essays.csv\n",
      "/Users/renitasequeira/Desktop/Python/LLM - Detect AI generated text/input/train_essays.csv\n",
      "/Users/renitasequeira/Desktop/Python/LLM - Detect AI generated text/input/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/Users/renitasequeira/Desktop/Python/LLM - Detect AI generated text/input'):\n",
    "    print(\"In here\")\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b015b",
   "metadata": {},
   "source": [
    "## Reading the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "e42d3e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= pd.read_csv(\"./input/train_essays.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "023732a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"./input/test_essays.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "991223a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0059830c</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005db917</td>\n",
       "      <td>0</td>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008f63e3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00940276</td>\n",
       "      <td>0</td>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00c39458</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  prompt_id                                               text  \\\n",
       "0  0059830c          0  Cars. Cars have been around since they became ...   \n",
       "1  005db917          0  Transportation is a large necessity in most co...   \n",
       "2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
       "3  00940276          0  How often do you ride in a car? Do you drive a...   \n",
       "4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
       "\n",
       "   generated  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "998dec2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>2</td>\n",
       "      <td>Aaa bbb ccc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111bbbb</td>\n",
       "      <td>3</td>\n",
       "      <td>Bbb ccc ddd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222cccc</td>\n",
       "      <td>4</td>\n",
       "      <td>CCC ddd eee.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  prompt_id          text\n",
       "0  0000aaaa          2  Aaa bbb ccc.\n",
       "1  1111bbbb          3  Bbb ccc ddd.\n",
       "2  2222cccc          4  CCC ddd eee."
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4629aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data = pd.read_csv(\"./input/train_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d2335af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>instructions</th>\n",
       "      <th>source_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Car-free cities</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "      <td># In German Suburb, Life Goes On Without Cars ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Does the electoral college work?</td>\n",
       "      <td>Write a letter to your state senator in which ...</td>\n",
       "      <td># What Is the Electoral College? by the Office...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_id                       prompt_name  \\\n",
       "0          0                   Car-free cities   \n",
       "1          1  Does the electoral college work?   \n",
       "\n",
       "                                        instructions  \\\n",
       "0  Write an explanatory essay to inform fellow ci...   \n",
       "1  Write a letter to your state senator in which ...   \n",
       "\n",
       "                                         source_text  \n",
       "0  # In German Suburb, Life Goes On Without Cars ...  \n",
       "1  # What Is the Electoral College? by the Office...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb4a8eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prompt_id                       prompt_name  \\\n",
      "0          0                   Car-free cities   \n",
      "1          1  Does the electoral college work?   \n",
      "\n",
      "                                        instructions  \\\n",
      "0  Write an explanatory essay to inform fellow ci...   \n",
      "1  Write a letter to your state senator in which ...   \n",
      "\n",
      "                                         source_text  \n",
      "0  # In German Suburb, Life Goes On Without Cars ...  \n",
      "1  # What Is the Electoral College? by the Office...  \n"
     ]
    }
   ],
   "source": [
    "print(prompt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3924285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('./input/train_prompts.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        print(' '.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50790a2c",
   "metadata": {},
   "source": [
    "### Function to add rows to the train_essays.csv file\n",
    "Text is generated on ChatGPT and the prompt_id specifies the prompt on which the essay is written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a54f28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row_csv(text, prompt_id):\n",
    "    import secrets\n",
    "\n",
    "    random_id = '{:08x}'.format(secrets.randbelow(16**8))\n",
    "    print(random_id)\n",
    "    \n",
    "#     text = \"\"\"\"\"\"\n",
    "    \n",
    "    # Assuming the new data\n",
    "    new_row = {\n",
    "        'id': random_id,\n",
    "        'prompt_id': prompt_id,\n",
    "        'text': text,\n",
    "        'generated': 1\n",
    "    }\n",
    "\n",
    "    # CSV file path\n",
    "    csv_file_path = './input/train_essays.csv'\n",
    "\n",
    "    # Open the file in append mode\n",
    "    with open(csv_file_path, 'a', newline='') as csv_file:\n",
    "        # Create a CSV writer object\n",
    "        csv_writer = csv.DictWriter(csv_file, fieldnames=['id', 'prompt_id', 'text', 'generated'])\n",
    "\n",
    "        # If the file is empty, write the header\n",
    "        if csv_file.tell() == 0:\n",
    "            csv_writer.writeheader()\n",
    "\n",
    "        # Write the new row\n",
    "        csv_writer.writerow(new_row)\n",
    "\n",
    "    print(\"Row added successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "778b3094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0638870d\n",
      "Row added successfully.\n"
     ]
    }
   ],
   "source": [
    "add_row_csv(text, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "25787126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fe6a46b2\n",
      "Row added successfully.\n"
     ]
    }
   ],
   "source": [
    "add_row_csv(text, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47336e2c",
   "metadata": {},
   "source": [
    "### Splitting the data into training and devlopment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9b643fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1254, 4), (139, 4))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = df_train.sample(frac = 0.9, random_state = 41)\n",
    "dev_data = df_train.drop(train_data.index)\n",
    "train_data.shape, dev_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53372060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0059830c</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005db917</td>\n",
       "      <td>0</td>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008f63e3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00940276</td>\n",
       "      <td>0</td>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00c39458</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  prompt_id                                               text  \\\n",
       "0  0059830c          0  Cars. Cars have been around since they became ...   \n",
       "1  005db917          0  Transportation is a large necessity in most co...   \n",
       "2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
       "3  00940276          0  How often do you ride in a car? Do you drive a...   \n",
       "4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
       "\n",
       "   generated  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d058a5",
   "metadata": {},
   "source": [
    "## Function to build the vocabulary using the nltk kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "332d6e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Need to download them only once\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def build_vocabulary_with_nltk(essays):\n",
    "    all_words = []\n",
    "\n",
    "    for essay in essays:\n",
    "        # Convert to lowercase for case-insensitivity\n",
    "        essay_lower = essay.lower() if isinstance(essay, str) else str(essay)\n",
    "\n",
    "        # Tokenize the essay into words\n",
    "        words = word_tokenize(essay_lower)\n",
    "\n",
    "        # Remove punctuation and numbers\n",
    "        words = [word for word in words if word.isalpha()]\n",
    "\n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        all_words.extend(words)\n",
    "\n",
    "    # Remove duplicates to create a set of unique words\n",
    "    unique_words = list(set(all_words))\n",
    "    \n",
    "    # Filter out rare words (occurrence less than five times)\n",
    "    filtered_words = [word for word in unique_words if all_words.count(word) >= 5]\n",
    "\n",
    "    # Sort the words for consistency\n",
    "    filtered_words.sort()\n",
    "\n",
    "    # Reverse index\n",
    "    reverse_index = {word: index for index, word in enumerate(filtered_words)}\n",
    "\n",
    "    return filtered_words, reverse_index\n",
    "\n",
    "\n",
    "# Extract essays from the 'text' column\n",
    "essays_from_dataset = train['text'].tolist()\n",
    "\n",
    "# Build the vocabulary for the essay dataset using nltk\n",
    "vocabulary_nltk_dataset, reverse_index_nltk_dataset = build_vocabulary_with_nltk(essays_from_dataset)\n",
    "\n",
    "# Test\n",
    "# print(\"Vocabulary List (with nltk) for the essay dataset:\", vocabulary_nltk_dataset)\n",
    "# print(\"Reverse Index (with nltk) for the essay dataset:\", reverse_index_nltk_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "319130ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4097"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary_nltk_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c844fd39",
   "metadata": {},
   "source": [
    "### Function to count the documents containing the words from the vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "081a226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_documents_containing_words(word_list, documents):\n",
    "    # Initialize a Counter to count the number of documents containing each word\n",
    "    documents_containing_words = Counter()\n",
    "\n",
    "    # Create a set for each word to efficiently check document presence\n",
    "    word_presence = {word: set() for word in word_list}\n",
    "\n",
    "    # Iterate over each document\n",
    "    for doc_index, document in enumerate(documents):\n",
    "        # Tokenize the document into words\n",
    "        words = set(document.split())\n",
    "\n",
    "        # Check and update the presence of each word in the current document\n",
    "        for word in word_list:\n",
    "            if word in words:\n",
    "                word_presence[word].add(doc_index)\n",
    "\n",
    "    # Count the number of documents containing each word\n",
    "    for word, document_indices in word_presence.items():\n",
    "        documents_containing_words[word] = len(document_indices)\n",
    "\n",
    "    return documents_containing_words\n",
    "\n",
    "#word list and the essays\n",
    "word_list = vocabulary_nltk_dataset\n",
    "documents = train['text'].tolist()\n",
    "\n",
    "# Count the number of documents containing each word\n",
    "documents_containing_words_count = count_documents_containing_words(word_list, documents)\n",
    "\n",
    "# Display the result\n",
    "# print(\"Number of Documents Containing Each Word:\")\n",
    "# print(documents_containing_words_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d5b0c",
   "metadata": {},
   "source": [
    "### Find the positive docs(LLM docs) count containing the words from the vocabulary list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "176d3c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_documents = train.loc[train.generated == 1]\n",
    "#print(LLM_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2a164d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_LLM_documents_containing_words(word_list, documents):\n",
    "    # Initialize a Counter to count the number of documents containing each word\n",
    "    documents_containing_words = Counter()\n",
    "\n",
    "    # Create a set for each word to efficiently check document presence\n",
    "    word_presence = {word: set() for word in word_list}\n",
    "\n",
    "    # Iterate over each document\n",
    "    for doc_index, document in enumerate(documents):\n",
    "        # Tokenize the document into words\n",
    "        words = set(document.split())\n",
    "\n",
    "        # Check and update the presence of each word in the current document\n",
    "        for word in word_list:\n",
    "            if word in words:\n",
    "                word_presence[word].add(doc_index)\n",
    "\n",
    "    # Count the number of documents containing each word\n",
    "    for word, document_indices in word_presence.items():\n",
    "        documents_containing_words[word] = len(document_indices)\n",
    "\n",
    "    return documents_containing_words\n",
    "\n",
    "#word list and the essays\n",
    "word_list = vocabulary_nltk_dataset\n",
    "documents = LLM_documents['text'].tolist()\n",
    "\n",
    "# Count the number of documents containing each word\n",
    "pos_documents_containing_words_count = count_documents_containing_words(word_list, documents)\n",
    "\n",
    "# Display the result\n",
    "# print(\"Number of Documents Containing Each Word:\")\n",
    "# print(documents_containing_words_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245ec5fa",
   "metadata": {},
   "source": [
    "### Probability of the occurrence of the words in the vocabulary list\n",
    "P[“the”] = num of documents containing ‘the’ / num of all documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "beaeb67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_documents = len(train)\n",
    "\n",
    "word_probabilities = {word: documents_containing_words_count[word] / total_documents for word in vocabulary_nltk_dataset}\n",
    "# print(\"Probability of the occurrence for each word:\")\n",
    "# print(word_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a2bda",
   "metadata": {},
   "source": [
    "### Conditional probability based on the class (human or LLM)\n",
    "P[“the” | LLM]  = # of positive documents containing “the” / num of all LLM documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4abfcb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LLM_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "9341aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_probabilities = {word: pos_documents_containing_words_count[word] / len(LLM_documents) for word in vocabulary_nltk_dataset}\n",
    "# print(\"Probability of the occurrence for each word:\")\n",
    "# print(word_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95ab73",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "42d685d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6157"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "0a1754e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/renitasequeira/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the development set: 0.989247311827957\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Load the dataset\n",
    "df_train = pd.read_csv(\"./input/train_essays.csv\")\n",
    "\n",
    "# Split the dataset into train and development sets\n",
    "train_data, dev_data = train_test_split(df_train, test_size=0.2, random_state=45)\n",
    "\n",
    "\n",
    "# Step 3: Build a vocabulary and reverse index\n",
    "\n",
    "# Download the NLTK stopwords if you haven't already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def build_vocab(data, min_occurrence=5):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    word_counts = defaultdict(int)\n",
    "    for essay in data['text']:\n",
    "        for word in essay.split():\n",
    "            # Exclude stopwords\n",
    "            if word.lower() not in stop_words:\n",
    "                word_counts[word] += 1\n",
    "    \n",
    "    vocab = [word for word, count in word_counts.items() if count >= min_occurrence]\n",
    "    reverse_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    return vocab, reverse_index\n",
    "\n",
    "vocab, reverse_index = build_vocab(train_data)\n",
    "\n",
    "# Calculate probabilities with Laplace smoothing\n",
    "def calculate_probabilities(data, vocab, reverse_index, e=1):\n",
    "    total_documents = len(data)\n",
    "    class_counts = defaultdict(int)\n",
    "    word_counts = defaultdict(int)\n",
    "    class_word_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        is_llm = row['generated']\n",
    "        class_counts[is_llm] += 1\n",
    "        for word in row['text'].split():\n",
    "            word_id = reverse_index.get(word)\n",
    "            if word_id is not None:\n",
    "                word_counts[word_id] += 1\n",
    "                class_word_counts[is_llm][word_id] += 1\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    prior_prob = class_counts[True] / total_documents\n",
    "    \n",
    "    p_class_generated_true = class_counts[True]/total_documents\n",
    "    probabilities = {}\n",
    "    for word_id, word in enumerate(vocab):\n",
    "        p_occurrence = (word_counts[word_id] + e) / (total_documents + e * vocab_size)\n",
    "        p_occurrence_llm = (class_word_counts[True][word_id] + e) / (class_counts[True] + e * vocab_size)\n",
    "        \n",
    "        probabilities[word] = {'p_occurrence': p_occurrence, 'p_occurrence_llm': p_occurrence_llm}\n",
    "\n",
    "    return probabilities, prior_prob\n",
    "\n",
    "probabilities, prior_probability_llm_generated = calculate_probabilities(train_data, vocab, reverse_index)\n",
    "\n",
    "\n",
    "# Calculate probabilities\n",
    "# def calculate_probabilities(data, vocab, reverse_index):\n",
    "#     total_documents = len(data)\n",
    "#     class_counts = defaultdict(int)\n",
    "#     word_counts = defaultdict(int)\n",
    "#     class_word_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "#     for _, row in data.iterrows():\n",
    "#         is_llm = row['generated']\n",
    "#         class_counts[is_llm] += 1\n",
    "#         for word in row['text'].split():\n",
    "#             word_id = reverse_index.get(word)\n",
    "#             if word_id is not None:\n",
    "#                 word_counts[word_id] += 1\n",
    "#                 class_word_counts[is_llm][word_id] += 1\n",
    "\n",
    "#     probabilities = {}\n",
    "#     for word_id, word in enumerate(vocab):\n",
    "#         p_occurrence = word_counts[word_id] / total_documents\n",
    "#         p_occurrence_llm = class_word_counts[True][word_id] / class_counts[True]\n",
    "        \n",
    "#         probabilities[word] = {'p_occurrence': p_occurrence, 'p_occurrence_llm': p_occurrence_llm}\n",
    "\n",
    "#     return probabilities\n",
    "\n",
    "# probabilities = calculate_probabilities(train_data, vocab, reverse_index)\n",
    "\n",
    "# Calculate accuracy using dev dataset\n",
    "def classify(essay, probabilities, prior_probability_llm_generated):\n",
    "    score = 1.0\n",
    "    score_denom1 = 1.0\n",
    "    score_denom2 = 1.0\n",
    "    for word in essay.split():\n",
    "        word_id = reverse_index.get(word)\n",
    "        if word_id is not None:\n",
    "            score += probabilities[word]['p_occurrence_llm']\n",
    "            score_denom1 *= probabilities[word]['p_occurrence_llm']\n",
    "            score_denom2 *= (1-probabilities[word]['p_occurrence_llm'])\n",
    "    return (score * prior_probability_llm_generated)/ (score_denom1 + score_denom2)\n",
    "#     return score\n",
    "\n",
    "correct_predictions = 0\n",
    "for _, row in dev_data.iterrows():\n",
    "    essay = row['text']\n",
    "    is_llm = row['generated']\n",
    "    prediction = classify(essay, probabilities, prior_probability_llm_generated)\n",
    "    if (prediction >= 0.5 and is_llm) or (prediction < 0.5 and not is_llm):\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / len(dev_data)\n",
    "print(f\"Accuracy on the development set: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "fd30716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words predicting LLM-generated essays:\n",
      "['Electoral', 'car', 'College', 'usage', 'urban', 'vote', 'candidates', 'electoral', 'limiting', 'cities']\n",
      "\n",
      "Top 10 words predicting human-generated essays:\n",
      "['congress', 'qualified', '538', '270', 'requires', 'elect', 'president.', 'held', '4', 'years']\n"
     ]
    }
   ],
   "source": [
    "# Sort the vocabulary based on conditional probability for LLM\n",
    "sorted_vocab_llm = sorted(vocab, key=lambda word: probabilities[word]['p_occurrence_llm'], reverse=True)\n",
    "\n",
    "# Sort the vocabulary based on conditional probability for human (not LLM)\n",
    "sorted_vocab_human = sorted(vocab, key=lambda word: (1 - probabilities[word]['p_occurrence_llm']), reverse=True)\n",
    "\n",
    "# Derive the top 10 words for each class\n",
    "top_words_llm = sorted_vocab_llm[:10]\n",
    "top_words_human = sorted_vocab_human[:10]\n",
    "\n",
    "print(\"Top 10 words predicting LLM-generated essays:\")\n",
    "print(top_words_llm)\n",
    "\n",
    "print(\"\\nTop 10 words predicting human-generated essays:\")\n",
    "print(top_words_human)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "0cd9f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "df_test = pd.read_csv(\"./input/test_essays.csv\")\n",
    "\n",
    "# Classify the test dataset\n",
    "def classify_test(essay, probabilities, prior_probability_llm_generated):\n",
    "    score = 1.0\n",
    "    score_denom1 = 1.0\n",
    "    score_denom2 = 1.0\n",
    "    for word in essay.split():\n",
    "        word_id = reverse_index.get(word)\n",
    "        if word_id is not None:\n",
    "            score *= probabilities[word]['p_occurrence_llm']\n",
    "            score_denom1 *= probabilities[word]['p_occurrence_llm']\n",
    "            score_denom2 *= (1-probabilities[word]['p_occurrence_llm'])\n",
    "    return (score * prior_probability_llm_generated)/ (score_denom1 + score_denom2)\n",
    "\n",
    "# Apply the classifier to the test dataset\n",
    "df_test['predicted_generated'] = df_test['text'].apply(lambda x: classify_test(x, probabilities,prior_probability_llm_generated))\n",
    "\n",
    "# Threshold for classification (adjust as needed)\n",
    "# threshold = 0.5\n",
    "# df_test['predicted_label'] = (df_test['predicted_generated'] >= threshold).astype(int)\n",
    "\n",
    "# Save the results for Kaggle submission\n",
    "submission_df = df_test[['id', 'predicted_generated']]\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee3bbb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
